var contents = new Array("To begin using Industrial Insight Studio, navigate to the following URL in your browser:\n\rhttp://74.248.112.32:9165/","Access the Platform","topics\\Access_the_Platform.htm","\n\r\n\rFor better insight into the predictions, you can build a visual dashboard:  \n\r\n\rIf needed you should create a new   dataset for dashbord shch as predicted data.  \n\r\n\r&nbsp;               Navigate to the Dashboard Page to create interactive graphs and other visualizations.\n\r  \n\r\n\r&nbsp;               \n\r\n\r             Customize the dashboard according to your needs and explore the data             further.  \n\r\n\r               \n\r\n\r             &nbsp;","Dashboards","topics\\Dashboards.htm","\n\r\n\rNew Topic (Press Shift-Enter to insert a line break. Press Enter to add a paragraph return.)\n\r\n\rresample\n\r\n\r\n\r\n\rdatapipeline\n\r\n\r","Data Pipelines","topics\\Data_Pipelines.htm","\n\r\n\rOnce the data is uploaded or a db connection is created:\n\r  Navigate to the   Dataset   Page.   Select the uploaded   file or db connection&nbsp;to create your dataset.   you could add   multiple datasources to a dataset and join them.\n\r   \n\r\n\r            ","Data Sets","topics\\Data_Sets.htm","\n\r  \n\r","DB Connections","topics\\DB_Connections.htm","\n\r\n\rinstall docker:\n\r&nbsp;&nbsp;&nbsp; sudo apt update\n\r&nbsp;&nbsp;&nbsp; sudo apt install apt-transport-https ca-certificates curl software-properties-common\n\r&nbsp;&nbsp;&nbsp; curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\r&nbsp;&nbsp;&nbsp; echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\r&nbsp;&nbsp;&nbsp; sudo apt update\n\r&nbsp;&nbsp;&nbsp; apt-cache policy docker-ce\n\r&nbsp;&nbsp;&nbsp; sudo apt install docker-ce\n\r&nbsp;&nbsp;&nbsp; sudo systemctl status docker\n\r\n\rinstall docker compose:\n\r&nbsp;&nbsp;&nbsp; mkdir -p ~/.docker/cli-plugins/\n\r&nbsp;&nbsp;&nbsp; curl -SL https://github.com/docker/compose/releases/download/v2.3.3/docker-compose-linux-x86_64 -o ~/.docker/cli-plugins/docker-compose\n\r&nbsp;&nbsp;&nbsp; chmod +x ~/.docker/cli-plugins/docker-compose\n\r&nbsp;&nbsp;&nbsp; docker compose version\n\r\n\rgit clone:  \n\r\n\rsudo apt update\n\rsudo apt install git  \n\r\n\rgit clone https://github.com/DevelopmentDigitalTwin/caldexML.git  \n\r\n\r(you need&nbsp;a credential)  Run manually\n\r\n\rgot to path/caldexML/tracking folder:  \n\r\n\rdocker compose --env-file .env up -d\n\r\n\rgot to path/caldexML/streaming folder:  \n\r\n\rdocker compose --env-file .env up -d\n\r\n\rgot to path/caldexML/scheduling folder:  \n\r\n\rdocker compose --env-file .env up -d\n\r\n\rgot to path/caldexML/serving folder:  \n\r\n\rdocker compose --env-file .env up -d  \n\r\n\r     &nbsp;Use a Single Script to Start All Projects\n\r\n\rIf you prefer one script to start all projects, you can use a shell script.\n\r    \n\r\n\rCreate a script:sudo nano /usr/local/bin/start-docker-ml-engine.sh    \n\r\n\rAdd:#!/bin/bashpath/caldexML/tracking/docker compose --env-file .env up -d docker compose -fpath/caldexML/streaming           /docker compose --env-file .env up -d docker compose -f    path/caldexML/scheduling/docker compose --env-file .env up -d docker compose -f path/caldexML/serving             /docker compose --env-file     .env up -d docker compose -f    \n\r\n\rMake it executable:sudo chmod +x /usr/local/bin/start-docker-ml-engine.sh    \n\r\n\rCreate a systemd service:sudo nano /etc/systemd/system/docker-compose-multi.service  \n\r\n\rAdd:[Unit]Description=Start Multiple Docker Compose ProjectsAfter=network.target docker.serviceRequires=docker.service[Service]ExecStart=/usr/local/bin/start-docker-ml-engine.shRestart=alwaysUser=userGroup=docker[Install]WantedBy=multi-user.target    \n\r\n\rEnable the service:sudo systemctl daemon-reloadsudo systemctl enable docker-compose-multisudo systemctl start docker-compose-multi\n\r","Developers Guide","topics\\Developers_Guide.htm","Go to the Upload File page. Ensure that your file is in CSV format.. Follow the instructions to upload the file successfully. \n\r\n\r                   \n\r\n\rIn this section you could select just needed&nbsp;columns and delete extra rows.        \n\r\n\rAlso you could change data type of columns and define format of date-time columns.       \n\r","Import Files","topics\\Import_Files.htm","\n\r\n\rTechnology stack\n\r\n\r\n\r\n\rArchitecture diagrams\n\r\n\r\n\r\n\rAnd combined flow:\n\r\n\r\n\r\n\r&nbsp;\n\r\n\r&nbsp;","Infrastructure","topics\\Infrastructure.htm","\n\r\n\rWelcome to Industrial Insight Studio, a powerful platform designed to simplify the process of building and deploying machine learning (ML) models for industrial applications. \n\rThis guide will walk you through every step, from data preparation to visualizing the outcomes of your model\'s predictions.        \n\r\n\r\n\r","Introduction","topics\\Introduction.htm","\n\r\n\rBefore training your model, Scheduling predictions and building dashboards, you need to upload&nbsp;a dataset file&nbsp;or make connection to a database.","introduction","topics\\introduction1.htm","\n\r\n\rNew Topic (Press Shift-Enter to insert a line break. Press Enter to add a paragraph return.)\n\r\n\rkjhk\n\r\n\r","Model Performance","topics\\Model_Performance.htm","\n\r  Go to the Pipeline Page.   Add the following to your pipeline:  \n\r    Numerical inputs and outputs (e.g., features like temperature, pressure,     etc.).     One transformer: Currently, the only available transformer is \"dropna\"     (to handle missing values).     One model: Choose either Linear Regression or XGBoost as your modeling     algorithm.     Other settings such as scaling, evaluation, and tuning parameters are pre-configured.\n\r\n\rTrain Model\n\r\n\rOnce your pipeline is set up, proceed to train the model:\n\r  Monitor the training progress via the Model Pipeline Status Page. This will give you   real-time feedback on the model\'s training state and any potential issues.\n\r  \n\r\n\r&nbsp;                   \n\r\n\r                   \n\r\n\rformula                   \n\r\n\r                   \n\r\n\r  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                   \n\r\n\r&nbsp;&nbsp;&nbsp; \'stdev\': statistics.stdev,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'mean\': statistics.mean,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'variance\': statistics.variance,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'pvariance\': statistics.pvariance,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'median\': statistics.median,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'math\': math,&nbsp; # This still allows access to math functions\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'sum\': sum,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'abs\': abs,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'round\': round,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'max\': max,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'min\': min,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'pow\': pow,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'sqrt\': math.sqrt,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'exp\': np.exp,&nbsp; # Use numpy.exp for array/series compatibility\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'log\': np.log,&nbsp; # Use numpy.log for array/series compatibility\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'log10\': np.log10,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'sin\': np.sin,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'cos\': np.cos,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'tan\': np.tan,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'pi\': math.pi,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'e\': math.e,\n\r&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'rnd\': random.random()                 ","Model Pipelines","topics\\Model_Pipelines.htm","\n\r\n\r              API Documentation:\n\r\n\rhttp://74.248.112.32:9180/docs\n\r\n\rusers could call trained models via specific API key \n\r\n\r&nbsp;","model serving","topics\\model_serving.htm","&nbsp;\n\r  Register: If   you don’t have an account, click on the \"Register\" link on the homepage and   complete the registration form.\n\r  \n\r\n\r\n\r                          Login: Once registered, enter your credentials on   the \"Login\" page to access the platform.\n\r                          \n\r\n\r                                         \n\r\n\r               &nbsp;                          \n\r\n\r               &nbsp;","Register & Login","topics\\Register_&_Login.htm","Upload Prediction Data\n\r\n\rTo make predictions, you first need to upload a prediction dataset:\n\r  Navigate to the Upload File page.   Ensure that your   file is in CSV format and under the 10MB limit.   Follow the steps to upload the data.\n\r  \n\r\n\r             &nbsp;Create Prediction Dataset\n\r\n\rAfter the file is uploaded, proceed to the Dataset Page to create the prediction dataset from the uploaded data.\n\r\n\rMap Training Model to Prediction Dataset\n\r\n\rFinally, link your trained model to the prediction dataset:\n\r  Go to the Model Schedule Page and select your trained model.   Assign it to the prediction dataset to   begin generating predictions.\n\r  \n\r\n\r&nbsp;           ","Schedule Predictions","topics\\Schedule_Predictions.htm");